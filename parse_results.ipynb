{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Experimental Results & Generate Latex Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, types\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_suffix = ['Edin', 'Glas', 'Melb', 'Osak', 'Toro']\n",
    "dat_names = ['Edinburgh', 'Glasgow', 'Melbourne', 'Osaka', 'Toronto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_all = ['\\\\textsc{Random}', '\\\\textsc{PersTour}', '\\\\textsc{PersTour-L}', '\\\\textsc{PoiPopularity}', \\\n",
    "               '\\\\textsc{PoiRank}', '\\\\textsc{Markov}', '\\\\textsc{MarkovPath}', \\\n",
    "               '\\\\textsc{Rank+Markov}', '\\\\textsc{Rank+MarkovPath}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latex Table for Recommendation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate results filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fname(dat_ix):\n",
    "    assert(0 <= dat_ix < len(dat_suffix))\n",
    "    \n",
    "    suffix = dat_suffix[dat_ix] + '.pkl'\n",
    "    \n",
    "    frank = os.path.join(data_dir, 'rank-' + suffix)\n",
    "    ftran = os.path.join(data_dir, 'tran-' + suffix)\n",
    "    fcomb = os.path.join(data_dir, 'comb-' + suffix)\n",
    "    frand = os.path.join(data_dir, 'rand-' + suffix)\n",
    "    fijcai = os.path.join(data_dir, 'ijcai-' + dat_suffix[dat_ix] + '.pkl')\n",
    "    return frank, ftran, fcomb, frand, fijcai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the F1 score for recommended trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_F1(traj_act, traj_rec, noloop=False):\n",
    "    '''Compute recall, precision and F1 for recommended trajectories'''\n",
    "    assert(isinstance(noloop, bool))\n",
    "    assert(len(traj_act) > 0)\n",
    "    assert(len(traj_rec) > 0)\n",
    "    \n",
    "    if noloop == True:\n",
    "        intersize = len(set(traj_act) & set(traj_rec))\n",
    "    else:\n",
    "        match_tags = np.zeros(len(traj_act), dtype=np.bool)\n",
    "        for poi in traj_rec:\n",
    "            for j in range(len(traj_act)):\n",
    "                if match_tags[j] == False and poi == traj_act[j]:\n",
    "                    match_tags[j] = True\n",
    "                    break\n",
    "        intersize = np.nonzero(match_tags)[0].shape[0]\n",
    "        \n",
    "    recall = intersize / len(traj_act)\n",
    "    precision = intersize / len(traj_rec)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the pairs-F1 score for recommended trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calc_pairsF1(y, y_hat):\n",
    "    assert(len(y) > 0)\n",
    "    assert(len(y) == len(set(y))) # no loops in y\n",
    "    n = len(y)\n",
    "    nr = len(y_hat)\n",
    "    n0 = n*(n-1) // 2\n",
    "    n0r = nr*(nr-1) // 2\n",
    "    \n",
    "    # y determines the correct visiting order\n",
    "    order_dict = dict()\n",
    "    for i in range(n):\n",
    "        order_dict[y[i]] = i\n",
    "        \n",
    "    nc = 0\n",
    "    for i in range(nr):\n",
    "        poi1 = y_hat[i]\n",
    "        for j in range(i+1, nr):\n",
    "            poi2 = y_hat[j]\n",
    "            if poi1 in order_dict and poi2 in order_dict and poi1 != poi2:\n",
    "                if order_dict[poi1] < order_dict[poi2]: nc += 1\n",
    "\n",
    "    precision = (1.0 * nc) / (1.0 * n0r)\n",
    "    recall = (1.0 * nc) / (1.0 * n0)\n",
    "    if nc == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2. * precision * recall / (precision + recall)\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load results data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(dat_ix):\n",
    "    assert(0 <= dat_ix < len(dat_suffix))\n",
    "    \n",
    "    frank, ftran, fcomb, frand, fijcai = gen_fname(dat_ix)\n",
    "    #print(frank)\n",
    "    assert(os.path.exists(frank))\n",
    "    #print(ftran)\n",
    "    assert(os.path.exists(ftran))\n",
    "    #print(fcomb)\n",
    "    assert(os.path.exists(fcomb))\n",
    "    #print(frand)\n",
    "    assert(os.path.exists(frand))\n",
    "    #print(fijcai)\n",
    "    assert(os.path.exists(fijcai))\n",
    "\n",
    "    # load results data\n",
    "    recdict_rank = pickle.load(open(frank, 'rb'))\n",
    "    recdict_tran = pickle.load(open(ftran, 'rb'))\n",
    "    recdict_comb = pickle.load(open(fcomb, 'rb'))\n",
    "    recdict_rand = pickle.load(open(frand, 'rb'))\n",
    "    recdict_ijcai = pickle.load(open(fijcai, 'rb'))\n",
    "    \n",
    "    return recdict_rank, recdict_tran, recdict_comb, recdict_rand, recdict_ijcai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate F1-scores from loaded results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(recdict_rank, recdict_tran, recdict_comb, recdict_rand, recdict_ijcai, func):\n",
    "    assert(isinstance(func, types.FunctionType) or isinstance(func, types.BuiltinFunctionType))\n",
    "    \n",
    "    # deal with missing values: \n",
    "    # get rid of recommendation that not all method are successful, due to ILP timeout.\n",
    "    assert(np.all(sorted(recdict_rank.keys()) == sorted(recdict_tran.keys())))\n",
    "    assert(np.all(sorted(recdict_rank.keys()) == sorted(recdict_comb.keys())))\n",
    "    \n",
    "    keys_all = sorted(recdict_ijcai.keys() & recdict_rank.keys())\n",
    "    \n",
    "    rank1 = []; rank2 = []\n",
    "    for key in keys_all:\n",
    "        rank1.append(func(recdict_rank[key]['REAL'], recdict_rank[key]['REC_POP']))\n",
    "        rank2.append(func(recdict_rank[key]['REAL'], recdict_rank[key]['REC_FEATURE']))\n",
    "    \n",
    "    tran1 = []; tran2 = []\n",
    "    for key in keys_all:\n",
    "        tran1.append(func(recdict_tran[key]['REAL'], recdict_tran[key]['REC_DP']))\n",
    "        tran2.append(func(recdict_tran[key]['REAL'], recdict_tran[key]['REC_ILP']))\n",
    "\n",
    "    comb1 = []; comb2 = []\n",
    "    for key in keys_all:\n",
    "        comb1.append(func(recdict_comb[key]['REAL'], recdict_comb[key]['REC_DP']))\n",
    "        comb2.append(func(recdict_comb[key]['REAL'], recdict_comb[key]['REC_ILP']))\n",
    "            \n",
    "    rand = []\n",
    "    for key in keys_all:\n",
    "        rand.append(func(recdict_rand[key]['REAL'], recdict_rand[key]['REC_RAND']))\n",
    "    \n",
    "    ijcai05T = []; ijcai05L = []\n",
    "    for key in keys_all:\n",
    "        ijcai05T.append(func(recdict_ijcai[key]['REAL'], recdict_ijcai[key]['REC05T']))\n",
    "        ijcai05L.append(func(recdict_ijcai[key]['REAL'], recdict_ijcai[key]['REC05L']))\n",
    "    \n",
    "    metrics = [rand, ijcai05T, ijcai05L, rank1, rank2, tran1, tran2, comb1, comb2]\n",
    "    means = [np.mean(x) for x in metrics]\n",
    "    stds  = [np.std(x)  for x in metrics]\n",
    "    \n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Latex tables from calculated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_latex_table(mean_df, std_df, ismax_df, ismax2nd_df, title, label):    \n",
    "    strs = []\n",
    "    strs.append('\\\\begin{table*}[t]\\n')\n",
    "    strs.append('\\\\caption{' + title + '}\\n')\n",
    "    strs.append('\\\\label{' + label + '}\\n')\n",
    "    strs.append('\\\\centering\\n')\n",
    "    strs.append('\\\\begin{tabular}{l|' + (mean_df.shape[1])*'c' + '} \\\\hline\\n')\n",
    "    for col in mean_df.columns:\n",
    "        strs.append(' & ' + col)\n",
    "    strs.append(' \\\\\\\\ \\\\hline\\n')\n",
    "    for ix in mean_df.index:\n",
    "        for j in range(mean_df.shape[1]):\n",
    "            if j == 0: strs.append(ix + ' ')\n",
    "            jx = mean_df.columns[j]\n",
    "            strs.append('& $')\n",
    "            if ismax_df.loc[ix, jx] == True: strs.append('\\\\mathbf{')\n",
    "            if ismax2nd_df.loc[ix, jx] == True: strs.append('\\\\mathit{')\n",
    "            strs.append('%.3f' % mean_df.loc[ix, jx] + '\\\\pm' + '%.3f' % std_df.loc[ix, jx])\n",
    "            if ismax_df.loc[ix, jx] == True or ismax2nd_df.loc[ix, jx] == True: strs.append('}')\n",
    "            strs.append('$ ')\n",
    "        strs.append('\\\\\\\\\\n')\n",
    "    strs.append('\\\\hline\\n')\n",
    "    strs.append('\\\\end{tabular}\\n')\n",
    "    strs.append('\\\\end{table*}\\n')\n",
    "    return ''.join(strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate evaluation data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#func = calc_F1\n",
    "func = calc_pairsF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[t]\n",
      "\\caption{Performance comparison on five datasets in terms of pairs-F$_1$ scores.\n",
      "    The best method for each dataset (i.e., a column) is shown in bold, the second best is shown in italic.}\n",
      "\\label{tab:pairf1}\n",
      "\\centering\n",
      "\\begin{tabular}{l|ccccc} \\hline\n",
      " & Edinburgh & Glasgow & Melbourne & Osaka & Toronto \\\\ \\hline\n",
      "\\textsc{Random} & $0.261\\pm0.155$ & $0.320\\pm0.168$ & $0.248\\pm0.147$ & $0.315\\pm0.127$ & $0.310\\pm0.167$ \\\\\n",
      "\\textsc{PersTour} & $0.417\\pm0.343$ & $\\mathbf{0.643\\pm0.366}$ & $0.216\\pm0.265$ & $0.468\\pm0.376$ & $0.504\\pm0.354$ \\\\\n",
      "\\textsc{PersTour-L} & $0.359\\pm0.207$ & $0.352\\pm0.162$ & $0.266\\pm0.140$ & $0.406\\pm0.238$ & $0.333\\pm0.163$ \\\\\n",
      "\\textsc{PoiPopularity} & $\\mathit{0.436\\pm0.259}$ & $0.507\\pm0.298$ & $0.316\\pm0.178$ & $0.365\\pm0.190$ & $0.384\\pm0.201$ \\\\\n",
      "\\textsc{PoiRank} & $0.432\\pm0.251$ & $\\mathit{0.548\\pm0.311}$ & $0.339\\pm0.203$ & $\\mathbf{0.511\\pm0.309}$ & $\\mathbf{0.518\\pm0.296}$ \\\\\n",
      "\\textsc{Markov} & $0.417\\pm0.248$ & $0.495\\pm0.296$ & $0.288\\pm0.195$ & $0.445\\pm0.266$ & $0.407\\pm0.241$ \\\\\n",
      "\\textsc{MarkovPath} & $0.400\\pm0.235$ & $0.485\\pm0.293$ & $0.294\\pm0.187$ & $0.442\\pm0.260$ & $0.405\\pm0.231$ \\\\\n",
      "\\textsc{Rank+Markov} & $\\mathbf{0.444\\pm0.263}$ & $0.545\\pm0.306$ & $\\mathbf{0.351\\pm0.220}$ & $0.492\\pm0.292$ & $0.512\\pm0.303$ \\\\\n",
      "\\textsc{Rank+MarkovPath} & $0.428\\pm0.245$ & $0.533\\pm0.303$ & $\\mathit{0.344\\pm0.206}$ & $\\mathit{0.498\\pm0.293}$ & $\\mathit{0.514\\pm0.297}$ \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "methods = methods_all.copy() \n",
    "\n",
    "mean_df = pd.DataFrame(data=np.zeros((len(methods), len(dat_names)), dtype=float), \\\n",
    "                       columns=dat_names, index=methods)\n",
    "std_df  = pd.DataFrame(data=np.zeros((len(methods), len(dat_names)), dtype=float), \\\n",
    "                       columns=dat_names, index=methods)\n",
    "\n",
    "for dat_ix in range(len(dat_suffix)):\n",
    "    recdict_rank, recdict_tran, recdict_comb, recdict_rand, recdict_ijcai = load_results(dat_ix)\n",
    "    means, stds = calc_metrics(recdict_rank, recdict_tran, recdict_comb, recdict_rand, recdict_ijcai, func)\n",
    "    assert(len(means) == len(stds) == len(methods))\n",
    "    mean_df[dat_names[dat_ix]] = means\n",
    "    std_df[dat_names[dat_ix]]  = stds\n",
    "\n",
    "ismax_df = pd.DataFrame(data=np.zeros(mean_df.shape, dtype=bool), columns=mean_df.columns, index=mean_df.index)\n",
    "ismax2nd_df = ismax_df.copy()\n",
    "for col in ismax_df.columns:\n",
    "    indices = (-mean_df[col]).argsort().values[:2]\n",
    "    ismax_df.iloc[indices[0]][col] = True\n",
    "    ismax2nd_df.iloc[indices[1]][col] = True\n",
    "\n",
    "if func == calc_F1:\n",
    "    title = '''Performance comparison on five datasets in terms of F$_1$ scores. \n",
    "    The best method for each dataset (i.e., a column) is shown in bold, the second best is shown in italic.'''\n",
    "    label = 'tab:f1'\n",
    "else:\n",
    "    title = '''Performance comparison on five datasets in terms of pairs-F$_1$ scores.\n",
    "    The best method for each dataset (i.e., a column) is shown in bold, the second best is shown in italic.'''\n",
    "    label = 'tab:pairf1'\n",
    "strs = gen_latex_table(mean_df, std_df, ismax_df, ismax2nd_df, title, label)\n",
    "\n",
    "print(strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}